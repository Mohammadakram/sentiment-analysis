All the Python files containing their respective code are named after the appendix that they are provided in the dissertation report for ease of finding and the name of the file is also self-explanatory about the code they contain.

1. Download all the datasets required for the training of the model.
2. Connect the drive to colab notebook while running the code.
3. Make sure that the datasets are present in the folder named "Colab Notebooks" in drive.
4. if running the code in jupyter notebook change the line of code and import it from the device.
5. make sure that the name of the dataset matches the name given in the code line.
6. make sure the names of all the rows and columns match.
7. when the codes are run then the code will store a bert fine tuned model state dictionary to the connected Google Drive so make sure there is enough memory for the storage of the models.
8. the links to the datasets that I have used are below
9. make sure to change the Batch size and num_workers according to available RAM and CPU Cores.

-----------DATASETS LINKS-----------

1. Chat GPT Sentiment Analysis -  https://www.kaggle.com/datasets/charunisa/chatgpt-sentiment-analysis
2. Twitter Sentiment analysis -  https://www.kaggle.com/datasets/kazanova/sentiment140
3. IMDB Movie reviews -  https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
4. Instagram Play store reviews -  https://www.kaggle.com/datasets/saloni1712/instagram-play-store-reviews
5. wikitext data for pre-training -  https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/
6. if the above link doesn't work for wiki text data then use this -  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip?ref=blog.salesforceairesearch.com


---------------------THANK YOU--------------------------